{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用卷积神经网络来改善计算机视觉的精度\n",
    "主要参考于[DeepLearning.ai课程](https://github.com/lmoroney/dlaicourse)\n",
    "\n",
    "在上一节中，了解了如何使用包含三层(输入层，隐藏层，输出层)的深度神经网络（DNN）进行识别图片， 并且尝试了不同大小的隐藏层，训练时期数等对最终精度的影响。\n",
    "\n",
    "为了方便起见，以下是整个代码。运行它并记下最后打印出来的测试准确度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.4994 - accuracy: 0.8230\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.3776 - accuracy: 0.8644\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.3396 - accuracy: 0.8767\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.3154 - accuracy: 0.8840\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.2986 - accuracy: 0.8897\n",
      "10000/10000 [==============================] - 1s 55us/sample - loss: 0.3426 - accuracy: 0.8775\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# 加载数据集(翻墙可以直接下载)\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# 数据预处理\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# 建立神经网络模型\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation = tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation = tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# 编译神经网络\n",
    "model.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy',\n",
    "             metrics= ['accuracy'])\n",
    "\n",
    "# 训练神经网络\n",
    "model.fit(training_images, training_labels, epochs = 5)\n",
    "\n",
    "# 在测试集上评估神经网络的性能\n",
    "test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以观察到，在训练集的准确率大概是89%，在验证集上是87%…不坏…但是你怎么做才能更好呢?一种方法是使用卷积。我不打算在这里详细介绍卷积，但最终的概念是：它们缩小了图像的内容，以集中于特定的、独特的细节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 卷积神经网络\n",
    "如果您曾经使用滤波器进行过图像处理。那么卷积看起来会非常熟悉。\n",
    "\n",
    "简而言之，取一个数组(通常为3x3或5x5)并将其传递到图像上。通过根据矩阵中的公式改变底层像素，您可以完成诸如边缘检测之类的操作。例如，如果你看过此[链接](https://en.wikipedia.org/wiki/Kernel_(image_processing))，你会看到一个为边缘检测而定义的3x3，中间的细胞是8，而它所有的邻居都是-1。在本例中，对于每个像素，将其值乘以8，然后减去每个相邻像素的值。对每个像素都这样做，最终你会得到一个增强了边缘的新图像。\n",
    "\n",
    "这对于计算机视觉来说是极好的，因为通常可以像这样突出显示特征将一项与另一项区分开，因此所需处理的信息量就少得多了，因为您只是在突出显示的特征上进行训练。\n",
    "\n",
    "这就是卷积神经网络的概念。在数据进入全连接层之前，添加一些层来做卷积，然后进入密集层的信息会更集中，可能也更准确。\n",
    "\n",
    "运行下面的代码：这是与前面相同的神经网络，但是这一次首先添加了卷积层。这需要更长的时间，但看看对准确性的影响！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "trainging_images.shape: (60000, 28, 28)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 243,786\n",
      "Trainable params: 243,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 85s 1ms/sample - loss: 0.4330 - accuracy: 0.8432\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 81s 1ms/sample - loss: 0.2919 - accuracy: 0.8927\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 82s 1ms/sample - loss: 0.2471 - accuracy: 0.9080\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 82s 1ms/sample - loss: 0.2149 - accuracy: 0.9198\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 82s 1ms/sample - loss: 0.1884 - accuracy: 0.9296\n",
      "10000/10000 [==============================] - 4s 378us/sample - loss: 0.2696 - accuracy: 0.9070\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "# 加载数据集\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print(\"trainging_images.shape:\",training_images.shape)\n",
    "# 数据预处理\n",
    "training_images = training_images.reshape(60000,28,28,1)\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images.reshape(10000,28,28,1)\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# 构建卷积神经网络\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    tf.keras.layers.Dense(10,activation='softmax')\n",
    "])\n",
    "\n",
    "# 编译卷积神经网络\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 查看模型总体参数情况\n",
    "model.summary()\n",
    "\n",
    "# 训练卷积神经网络\n",
    "model.fit(training_images,training_labels,epochs=5)\n",
    "\n",
    "# 在测试集上评估模型性能\n",
    "test_loss = model.evaluate(test_images, test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练集数据和验证集数据上，精确度已经上升到93%和91%。这是朝着正确方向迈出的很重要的一步\n",
    "\n",
    "尝试运行更多的阶段：比如20个，然后观察其结果!虽然在训练集上结果看起来可能很好，但在验证集上结果实际可能会下降，这是由于所谓的“过拟合”，这将在后面讨论。\n",
    "* “过拟合”发生在网络从训练集很好地学习了数据，但它太过专注于训练集的数据，导致在测试集或者其他数据集上的结果精确度较低。例如，如果你一辈子只看到红色的鞋子，那么当你看到一只红色的鞋子时，你会非常善于识别它，但是蓝色的鞋可能会使你迷惑。\n",
    "\n",
    "**代码讲解：**\n",
    "* 1.预处理数据。 您会注意到这里有些变化：需要重新调整训练数据。 那是因为第一个卷积层期望一个包含所有内容的张量，所以我们不能用一个列表中的60,000个28x28x1项，而是要有一个60,000x28x28x1的4D列表，并且对于测试图像而言也是要满足同样的要求。 如果不这样做，则在训练时会出现错误，因为卷积无法识别数据形状。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "training_images = training_images.reshape(60000,28,28,1)\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images.reshape(10000,28,28,1)\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.构建模型。现在最上面不再是输入层，而是要加一个卷积层。参数是:\n",
    " * 1.您要生成的卷积数。 纯粹是任意的，但最好从32左右开始\n",
    " * 2.卷积的大小，在这个例子中是3x3网格\n",
    " * 3.要使用的激活函数：在本例中，我们将使用relu，它等价于当x>0时返回x，否则返回0\n",
    " * 4.第一层的形状是输入数据的形状。\n",
    "\n",
    "* 然后将在卷积后面跟随一个MaxPooling层，该层然后被设计为压缩图像，同时保持被卷积突出显示的特征的内容。 在本例中，通过为（MaxPooling）指定（2,2），效果是将图像大小缩小四分之一。 其思想是创建一个2x2像素阵列，并选择最大的像素阵列，从而将4个像素变为1。它在整个图像中重复此过程，从而将水平数量减半， 并将垂直像素的数量减半，有效地将图像缩小了25％。\n",
    " * 您可以调用model.summary()来查看网络的大小和形状，您会注意到在每个MaxPooling层之后，图像的大小都以这种方式减小了。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3.再添加一个卷积层"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4.现在将卷积层的输出展平。此后将拥有与非卷积版本相同的DNN结构"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " tf.keras.layers.Flatten(),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 5.和前卷积的例子中一样，有128个稠密层和10个输出层\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 6.现在编译模型，调用拟合方法进行训练，评估测试集的损失和准确性。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 可视化卷积层和池化层\n",
    "以下代码将以图形方式向我们展示卷积层输出的图像。\n",
    "* print (test_labels[;100])向我们显示了测试集中的前100个标签，您可以看到索引0、索引23和索引28处的标签都是相同的值(9)。\n",
    "* 让我们看一下在它们上面运行卷积的结果，您将开始看到它们之间出现的共同特征。\n",
    "* 现在，当DNN在这些数据上进行训练时，它会用更少的时间工作，它可能会在鞋子之间找到一个共同点基于卷积和池化的组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n",
      " 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n",
      " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAD7CAYAAABHYA6MAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO29eZQkZ3Xo+bsRudVe1Xuru6WWhMBqAQJJCIH8bAFPIGOMPNjwkAcPzODH+BnPQQfPgOCdZzw+5jyBZzisfrZsNBI2m7AQyFgsskA0AiSr1Whvqbsldbeqt9r33CLizh8RVZ1dmVmVmZWVS9X9nVMnM7/4Ir4btzLv98X9vu9eUVUMwzCM1sNptgCGYRhGacxAG4ZhtChmoA3DMFoUM9CGYRgtihlowzCMFsUMtGEYRouyIgMtIteJyLMiclhEbqqXUIZhGMYKDLSIuMCXgN8C9gA3iMieeglmWAdoGOud2ArOvRI4rKrPA4jIN4DrgafLndAbS+mWRNcKmmxvnkuPjajq5krqFnSA1wKDwMMicreqltSv6bZy3ULY+QGfA1zgH1T15mXqr/cdXVXptxpMt+V1uxIDvQN4seDzIPDapU7YkujiM7923QqabG+u/9XXjlZRvaoO0HRbuW6r7fzO4K5ExDbHr+a7WwOm21KsxActJcqKekIR+YCI7BORfVNeZgXNrTtKdYA7miTLWmOh81PVHDDf+RlGS7ESAz0I7Cr4vBM4sbiSqt6iqleo6hW9sdQKmlt3LNsBWudXMxV1foX6bZhkawCbO6kfKzHQDwMXicj5IpIA3g3cXR+xDCroAK3zq5mKnv4K9dsAmdYEtnigvtRsoFXVA/4U+CFwALhDVZ+ql2CGdYCrSEVPf0ZNmPuojqxkkhBVvQe4p06yGAWoqici8x2gC9xqHWDdWOj8gOOEnd8fNFekNUPViweM8qzIQBuri3WAq4N1fqtKxYsHgA+svjjtjRloY11ind+qUfHiAeAWsHXQS2GxOAzDqCc2d1JHbARtrDqVDpCcEvUGOmeLylRLPUUbrYC5j+qLGeg6I6IlDYg9xRnrBXMf1Q8z0Ctg3ujOG+SE65FwfQByvouvDjEnIO74TZPRMIz2xQx0GRaPhBcbYxFdeCQPonLXUVKJHKqCrw4EoXGOx7ySj++GYRhLsS4NtIjiSoDrKAnXQ0QJAocAwUFxnABVIZ2PkwsiFS12W6jgRzbX88NzXScg7sXI+zGOTvUx68XYkkqztWvGXByGYVTNujTQcccn4fok43n6u6dxnIC8F8PzYriuT8z18QOHockB5tIJAIJoeee8YfeDcJQcqJD2YuRVCFRwJWAi28HDo52cTPtcvrGb3mQWV4Jm3nLD2HPukaIyzyv+ms3MFodGnUgXl/lB8UKj6YxtazfWB+vSQIuEo+hAhUw2iYiSziVJ5xMkXI+uZBh4KHRbBAQqBJGhcAU0Ggw7aDjqFsUFfBXSXpyZfJzxnDLipxnOdDM014XrrA8DbRhG/ViTBjoo2MykCkHknlCV0L0ROLiOw9hMJ89O7mAy7/LCDAzmZtke7+Tl/dAX9zmve5rNXTPM5RKMZzrw1cERxRGfuBPgOgFSsEnq5Fw3z052czrj8Ij3PKeCAxyd2cXe6W2ILTk3DKNK1qSBXsz8xF6AQOSKAJjJJ3hq0uV0Ns+j7GM0/Sv62AMTv8mWVJytHTE6E1k83yVAmB8DByrEnICE6+NKQMz1EVFenOnlxTmHoYzHKe8Ac9kjzGWPWBQewzBqYk0aaIf51RVnT+wlXB8HxQscJtIdnM4kOZpJc9w9wWxuGIB0fowDzilGpjdwblcnA8leNnVP88aLDuEmvLNWcYgT4GUSnDh+DuNz3ZzOJDgwN82oM0rOm2zsTRuGseZYkwZ6Hgclr+EKi7jj0xHL4zoBw7NdDGdTHJ2N84T+lOmZ51Ci9cveaZ7x/5Xn3A3snHgHffE+LtoxSO8nduFvfiXO1BGc9DiSS+PMTeEeP8rMbT2cmurn2KzLI7l/xfcnF6631ulJnp0o4F+eemVRnQ8f/vuiMi+4vahsdviBorKfvXWiqOxtj3yrGhENo21Z0wZ6nqDArQEw7cU5nU4wmlGy/jSKV1BbUc3h+dOM5XxOpBOcHNnM+U/sJdj8As7EGGQzkMlB2sM71cnJkc0MzvQwmg3wg8XXMwzDqI01a6D9yCD76qAavuZ8F/Vi7Bvt5Pvpp5j2h8h5IyXPDzTDz7x/4/Gxc/jhyHm87P3vIelCPoBAQVH8ALKBcsybZMo5zZB/GNV8I2/TMIw1zLIGWkRuBd4GDKnqy6OyDcA3gd3AEeBdqjq+emLWxvzIOUAQBS9w8AOH43Meg7M/pUSY2gJ8ZrPPMZt9jhP8jH3pRkltGIYRUsnar9uA6xaV3QTcp6oXAfdFn1uCcLXFGZdGKubRH/lJnxjv55fD/byoY80UsWJE5IiIPCEij1riUsNYfyw7glbVvSKye1Hx9cA10fvbgfuBj9ZRrqpYHCdjfiOJqtARy9GTSjOdS/Lvo3BYjzPoP8nSo+eW4g2qWtoPUycq3Ya+o7+4Y8sv2iVYakKwFDHnvRXVM4z1TK0+6K2qehJAVU+KyJZyFQtT22yOd1bVSOEyuVJL5+bLCtc5zxvneLRzzw+EuVySmXycST/HlDOCF5i/wjCM+hOPba76nLx3quyxVZ8kLExt85LOjRUN1eaNsBbU9qOywtgMhSM/RxQ/OLOkrjeVJhHzODHZz7OTXRyZSXDQeZiR9AG0fQy0Aj+KUgL9XaTLBVbS+RmG0frUaqBPi8j2aPS8HRiql0CLN5cEi6LIzfuXHVGcQgOuZ3zPIpCIeSRjebKBy0gmzmgWZrwhgmC6XqI2gqtV9UT0hHKviDyjqnvnD9bS+RmG0T7UaqDvBt4L3By9freWiyw2xqUojKMcaBgOFAnf+1oYrzmcEOyK5wCYmOvEC1yenOjhp6OzjDpjZPMtt9BkSVT1RPQ6JCJ3AVcCe5c+y6gEETkCTAM+4KnqFc2VqLGUexTPe8Mruq6I7AK+AmwjDJV+i6p+bkUXXcdUsszu64QTgptEZBD4BKFhvkNE3g8cA965UkHmgxo5okgZu+2ILhhlVcjrGXeHo5BwPAY6Z8h6cY5MbGA8m+TRcZ8HM99ENUsbTQwiIl2Ao6rT0fs3A39Z3TWK79fXEgt3SqToumTXkaKy6ZnuorKr9v6gGpFajVWfgF2HeMCfqep+EekBHhGRe1X16WYL1o5UsorjhjKH3rTSxufDdRa3eeZ9YSQ6ODPqFoE4Qfjq+FGUOQ0nBLNJjs12MpSJMRSMR5tH2sc4R2wF7pKwt4oBX1PVtraGxtonWjwwv4BgWkQOADsAM9A10PSdhAurM0TOvFdBC3zKfoE7wxXFQUm4YWyNuBvQ1zFLMpFjeKqfFyY2cHwuxbeGpnhBH2M2dxraMC6Gqj4PXNpsOdYwS07AwtmTsEb1RMtzXw08VOKY6bYCmm6gFzNvnM+8D0fPC/kBIxdI3AlIxHwSrkcykSMRz+MHDmO5BMPZGC/yDJNp67SNsiw5AQtnT8KK5SyrChHpBu4EblTVqcXHTbeV0RQD7RcEL9ISqzTmmXdbxKNIzCJKXypDRzxHKp6jsyNNLh/n0OntDGc6ODidYv9EhnEZYio32LgbagFirs+m7rNXqFxwwQtF9To3FodBHXpuV1FZ3+biTSkvvfvBFUjYWtgE7OohInFC4/xVVf12s+VpZxpuoAMEP3DwoiBG8wbZoXhycN637IriOgExJ2BTzxRdnbMkUllSXWlmxns5ONXDExMuj+VP8EzmX9vV52w0iHpMwLYijhTndAQIdK6o7BXxa0vW3e99bUUySDhp8mXggKp+ZkUXMxpvoB2UmBsQi0bF837nhOvhOopEPmZgIVOJ57thhm3f5bnhLfjqLMR3nsh2cGg6xmA2zYQzhKqPGWdjGWwCdvW4GvhD4AkReTQq+7iq3tNEmdqWhhpoESXhesTcIDLIAclYHscJ6O6aI5HMhlm143lwFMcNcFyfiaGNPHdiByOZFHcd6+IRfRYlIK8ZfPLM5E/j+Wn8YJZ2nBA0GotNwK4eqvoAVLDBwaiIxhpowuVxoTsjHOXOb90OgvL/U993F7JlH/MmOZ59GNUsqrlGiG0YxiqyKbaFdwy8u6pzbhn+UtXt3Lj9g1XV/6u9z1XdRvKxn1d9Tvz3yx9rygh6MtvB1EyC0WySpyYSzHqKK0JMQgPuRrbakdCoz3jK6WyeGc1ySB9GgzRqboxl2fGPxROlpbe6W1pbw2hFGm6gY67PbD7O8bkOnpuOcefsT5nKHo2MrqWKMgzDmKehBnoml+CBEzvCHX4Z5WQmS8afCN0VNiI2Wphyj+G1PGpXSrlH8k9/7fsly3M7X1qyXG99pKgsdfm/lKy71OO20XgaaqBP5qf4qxM/JFAPP8ii6uH5U9jEnmEYRjENNdCBZpnLHmlkk4ZhGG1Ly231Nmrj2ZlJrv6ZLTU1jLVEJUljDcMwjCZgBtowDKNFqSRgf8kMCSKyAfgmsBs4ArxLVdsrZYlhVMiIN1RyxcYrOkvnqrhuoDhjybsuOlyy7tatpTPGfe3fSy87Tbzh+TJSli7/wWt+r6jsP37y7WWuYdnWW4lKRtDzGRIuBq4CPigie4CbgPtU9SLgvuizYRiGUScqyahSLkPC9YSpsABuB+4HProqUq5hRORW4G3AkKq+PCqr+unkwo4NfObXrjur7OdDG4vqffr439RD7Kr47qv/U1GZXyLN1jse/UYjxDGMtqEqH/SiDAlbI+M9b8S31Fu4dcJtwHWLyuzpxDCMypfZLc6QIOUyuxaft5DaJiFdXJ46M5rKkmfKmSQraYayz5DNNysmhLCh85Vs5yXESZDUOA7zORCVE+4gg7MPrEpwJlXdG3V8hdjTibFumA3y/HK2tB++HA9f8+aq29m6tbpARt0XPbp8pUV4we1VnwPvKXukIgNdJkPCaRHZrqonRWQ7UFLDhaltLujYpH9ywZkt3dP5Dl6Y7WcyJ/yIOEeaZKBF4lzBa/nNLUJHLGBDIocrGkWshgeGLub/Sz+G5482SqSznk6itEwl5D7T+W2OdzZKNqOAJ+a+Vaa8uOyvj1d79eoNRCmue/jO4kKnRJnRclSyiqNchoS7Cad8b45ev7vctRxRehPZgszcykA+hisu22e2MZTcXcMtrJy408mmZIz+RJak69Mdz+E6YUeiCgMJZUPqQma8nhW1M5etPnzhUhR2fi/p3GjBTAxjjVHJCLpkhgRCw3yHiLwfOAaUXm9UAgclQEi5Hrs60+QDh4FEkjfmf6ta+euCA2zryLIhmSUmwYJxhjD86a/1zvEnscvILxGzuhI++eIXK61a0dPJcly9pXjE/90txRN2zcAtkSe01GRipVz/q5WlajKMVqSSVRxLZUh4U60NOyiOo8QTWQA2ptK1XmrV2ZhK10e+FyuuWfXTiWG0EiLiAvuA46r6tmbL067YTsImIyJfB34JvExEBqMnkpuBa0XkEHBt9Nkw2okPAQeaLUS7Y8GSmoyq3lDmUM1PJ4bRTERkJ/DbwCeBDzdZnLbGRtDGmkVEbhWRIRF5sqBsg4jcKyKHoteBZsq4Rvks8BFYWAhVhIh8QET2icg+T7ONk6zNMANtrGVuwzYBNRQRmd8VW5zGpQBVvUVVr1DVK2KSbJB07YcZaGPNoqp7gbFFxdcTbv4hev3dhgq19rkaeLuIHAG+AbxRRP6puSK1L2agjfVGxSEKCh/DGyZdm6OqH1PVnaq6G3g38GNVLb9VzlgSmyQ0jDIUbgQSKbFw2zBWGRtBG+uN09HmH1ayCchYHlW939ZArwxRbdzAQESGgVlgpGGNrg6bqO0ezlPV4kjudSDS7dHoY63ytRLV3kNJ3UaBqL5XEMr1r4FRVb1ZRG4CNqjqR5a7+BrUbyUU3mejvrvl2m8GjWq/rG4baqABRGSfql7R0EbrTKvfQ6vLVwn1uIdoE9A1hD+008AngO8AdwDnEoUoUNXFE4mrLls70Oz7XO/tg/mgjTWMbQIy2h3zQRuGYbQozTDQtzShzXrT6vfQ6vJVQivfQyvLVk+afZ/rvf3G+6ANwzCMyjAXh2EYRotiBtowDKNFaaiBFpHrRORZETkcrUFteURkl4j8REQOiMhTIvKhqLzloqK1o36hfaLOtat+l6PZ+l9OrxLy+ej44yJyWR3bLvn7XlTnGhGZFJFHo78/r1f7y6KqDfkDXOA54AIgATwG7GlU+yuQeztwWfS+BzgI7AE+DdwUld8EfKrJcralfiPZfwO4DHiyoMz0uw70X4legbcC3yfM7HQV8FAd2y/5+15U5xrCzU4N/980cgR9JXBYVZ9X1RxhpKvrG9h+TajqSVXdH72fJswSsYPWi4rWlvqFtok617b6XY4m678SvV4PfEVDHgT657frr5Qlft8twYoMdJWPfDs4OyvfIC2kiEqItg2/GniIKqKiNYi21+8iTL/NpVH6r0SvDdH9ot/3Yl4nIo+JyPdF5JJ6t12Omg10lBTyS8BvET7y3yAie5Y6pURZ26zxE5Fu4E7gRlWdalCb1XSAba3fNsD0uzpUotdV1/0yv+/9hPEyLgW+QBguoCHUvA5aRF4H/IWqviX6/DEAVf3v5ep3u8lfbIp31ypr23MkMzqiFQaciTrAg4RJYweBh4EbVPXpUvV7Yik13VYezEdErgM+R+gD/QdVXTIxr4UbxVfVuoeGiOzIL+p93XpwQceGqs95Pj1eQ0ta9ru7EoWXeux47eJKIvIB4AMASSfGX1ywJtx2NfG+p28tFbGrHAu+OQARmffNlTTQm+LdptsKKXj6W+j8ROTucp3fGdyViNjm+KsV1e3h8KX1dPvpl7616nN+/7E7amgpV/a7uxIfdEWPHVqQe6zHTa2guXXHevN5NpI1O+G3ipxcjYuqqrca110rrMRADwK7Cj7vBE6sTByjgGU7wMKUTNN+pkFirQkq6vws5dVZ+JVWXKvrxZvBSgz0w8BFInK+iCQI84/dXR+xDCroAO3ppGaqfvprgExrghoWDxhLULOBjh5N/hT4IeHawTtU9al6CWZYB7iK2NPf6mHuozqyollZVb0HuKdOshgFqKonIvMdoAvcah1g3Vjo/IDjhJ3fHzRXpMr550tL5yEYnC29iufGw7eWKK3YY1EtVS8eMMpjGVVaGOsAVwfr/FaVit1HWMb0ZTEDbaxLrPNbNcx9VEcs3KhhGPXE5k7qiI2gDSp9wlQtfnp982Vnr0Db/M5TRXX8Z2eLyvb+c/EmgJlcoqjsqlc+XlQ2MVJih9cyW0yMxmDuo/qybgy0iOKK4qCIgBMZpfCzoioECIEKfmSIwrKzmTdShUZt/jFEousDBCVdcYax9jH3Uf1oOwNdaBhLGctSozyATtdjV/8YqWSWZCJHMpUlkczR0T+F4wbkZjvwsgnSsx2MjG0g58eYzSbJ+THygUPOdwkQFAgANzL4MQnojOeIuwGdiSxdqQxBIOS8OHk/3L7q2ByIUQP5H5beOBp/y9dXfO139P6XkuXfnvofK762UT/azkAXUs3kb8L12TgwTlffNB19M8R7Zon1zuHuFkglYfg4/nic3HAfHIJMJoXrdDObTeH6Lr46EIAfjYwdlKTr4YrSncySjOXp7Z6hb2ASVWF2uhvPc6uW0zCM1aFch1eOWjrCch3fUnx76otlj7WdgS4cIQfR+1Ij1HmjOJDM0N8xhyPK0NhGdHQTc7kks/kEKTfPQOcsrhMwl0uSzidQDa/riLKpe5qBTUPE43niyRyOhO4QcXShDd9zmZrsJZNNMjiyhYePXkA+cJjJx/FV2JTKsKWj2AdrGIaxHG1noOeNsl/gK445AQ5nDLUs+JdhR/8Y5+85xOSpTfzwiUs5NpviF6NZHtG9BBrgSqgCjbzN57h7+M3OnWzrCHjXpmF2X/kE8V0zeFdcid+1Eb97M8S7kewEzuww8VMv0PXVKU4e2clDL1zIF4cfJe2Pkw/SALzGvZZf37gVt4VH0aXcQkGpCcFX7y8q69k1dNbn+PWjRXX+eMsHi8o+dbCrqKyrtzgO+sG3DRaVffnRVxaVwU9LlBlGe9N2BrqQoHAyTxSikS9AwgmISUAQOGQmupma6uFUJsGJtDDonGJ69lDJa55MOZzOnIMjDqNTfWwfGkDiHrFTx3A6x3C7h9FECiczizMziZw+wfTYSxmd6uNU2mUs+xyeP4lqHoBTXUOcypyLKzZpaBhGdbSdgXZECaLVFQurLYJwHcX8xF1HzOPc/jG6Umn2DZ7H3z55EeO5gCf9QSY4zUT2WNnrz+RO8FP5GR2zfTz5q5ex9cm30hkT+hPgStiGI+AH4KmQC+DYrMdkkOMF5/EC4xx2FEeyDzPJ8GqrxViDxN9yfNWubZOB7UHbGWiIXBkFS+AWP447KP3d0/T0TnP84Mu4c/anpL0xcvkhlKXDzwbBNBPpJ5kATvJzSK9M1px3itNe8dpgwzCM5WhLAw2hfzkWjaYhXPrWFfPoS2ZRhYeOXEjGd/nVmJD2xvD8WdRSyBmG0Ua0rYGe33jiioauDhX6kll2bxzm1EQ//3Coj0d5mvH8UbL5k1h+z/KUWgZ45bkvFJVNjvcVle38WvFOv8X87dCXisv6KxSuJA+s5GTDaBva1kA7nDEsiWj5m4OSzcWZzScYDdJMBC+S8yYx42wYRjvSvgZalDgQdwI2d87SmcgyMtvNQyd3cnwuwTHnMdKZUwurKQzDMNqNZQ20iNwKvA0YUtWXR2UbgG8Cu4EjwLtUtZZ84xUzP1ouXLPrOgExJ6Anlaanc5bT070MziU4MSfM+iOoWp4+ozk4Tk9RWRBMN0GS0mQ/Gy9ZnrzRBjStRCUj6NuALwJfKSi7CbhPVW+OkkLeBHy0kgYX+zvLxTstF6QIwlUbXfE8fcnQAA/N9HJisp9Hx/t4aCzLqEwwly+XJV7Y0vUaLgouYVrmGOQZMv4kmfxoS/2AAETkCDBNmP7Cs9x4hrG+WNZAq+peEdm9qPh64Jro/e3A/VRooKF09LdCAgSXM0Z58XK6AOiK5zhnwwjZbJIHj5/Li3MJHhzL8kD2DjRIl11OJ7hcwat5+7l5TmUG+PHQFZyOjfAijzGXbS0DHfEGVS3X2ywgULRb0ZXF3VzpKHtvvOyRorLJ0YGisov/9edFZZlPn52s9lu3v6uoznS+eLT2vv/5W8XXGi5uc8MXjxaVGcZ6oVYf9FZVPQmgqidFZEu5ioW5xzbGw+298xtMKBN57uzzz2zfjosSi3YIJlyfXC7BXC7JUCbO8TlhVCbKGmdHutjUeQm9bGZnZ4yUm6HDDRiIJQi8jczFLzjTJmGQI1diOBIn5iTpkD4cHPzo2pPecaYzz7OKud0MY11w2Q6HX/wfHVWd0/nx6k1XtRt/7n3t/1R1G79xwz9UfU7yxvLHVn2SsDD32O6OzeoHDnkVvMAJd+L5zsKuQNUzu/VcR0k5AXEnNMadrkfC9Tmnb5zenhnGJ3s5Nr6RU3Nd/HgkwxP6C9LeWNmR846u1/L/XLCdl209wcnxUxyf7SHlBry83yUXpHi1fzFZ/2JiDsQkXGPdmwjocAO2dWR42cYhYq5PJt9F3nf53tE9fHH4O+S8kbN2DtZbfcCPopxtfxfpcoFSnZ9hGGuHWg30aRHZHo2etwNDy54REQB+4JAPHHwVMoGDH0WQ8yMDnXQCXBXiorhEm1LcgGTMo6tzju7eaaamu5nIphjOxjnqPsPUzLNLttsbDPCq815g1+VP4fzyVZyc7SHpBGxMevhB4cRj2DG4TsDGZIbuROhK2f2KgziJHPnpLvxcnIPjG4mNdpBb3axhV6vqiegJ5V4ReUZV984fLOz8LujYZGsJDWONUauBvht4L3Bz9PrdSk5yUJLRqLiL0FB3BQ6BRoHwVUi4Pj2xPAnXZyCVpiuZoSOVoadnBi8f48CL53H88MsYnEvy1KQy6mUZ9Z9ftu1BPcCXHn4DFxy4hJf0TXD5riNksklOT/eR9WIMZzqYyMdwNFwdknQCuhM5+jtmOTayhR9/56VkfIdsEEbR2zcakPXGVnP0jKqeiF6HROQu4Epg79JnGZVQ7QRsucfw1E2lYwE0Y8K53CO55xcPIpI33rkqMojILsIFBdsIx2O3qOrnVqWxdUAly+y+TjghuElEBoFPEBrmO0Tk/cAx4J0VtSZhaND5QPcBctboVVXojOfY2DVDMpFn48ZROvunSfTOkNw6Tvb0AN96/FLuPp3muPscx2cfjAzk8n7gyfQBvpA5jDvcw6fPfwfXvOXH5Ke76D58HjNzncyNbGUoG8eNfN0Jx6cnlaa3e4YHT+zivw3ez0z2ReanLFX9itqtFRHpAhxVnY7evxn4y3L1+3qm+e03nB1y8/4HXldU7zdf/2BRWc8lLxaVnfPByu4t9ZGzlzLO/d/fKaoTvyhXVHb4q5cXlZWahFxlKpqANarCA/5MVfeLSA/wiIjcq6qWNbIGKlnFcUOZQ2+qujEJ2Ngxhx+Euf+yfozxbIp84JD2XbKBkHID+ue6Sbg+W6b66EmlSSVy9HRPMz3Tw5HZGMPOMNP+UMXGOboTVHP4wTQvzCR4/pGXoyrMZTrIeTEGkhni0eRjdyIMzj86283JqT6em0mQ8SYava56K3CXhGFKY8DXVPUHjRTAMKolWjwwv4BgWkQOADuwtL410dCdhB2pDBdf8BxjYwOMTPYzlunk4dFORrIBh70xjnMQEYc4KQSHuCRxNU6MTuLaT15ynPR/xFxuiECz1DKCVc1z+8S/8aMHfo3zdCtv2Q7bO+Z4zYWHOOcVh9BACHIJ5kb7+MyP3sjXJp9myttP3hurv0KWlFOfBy5taKPriyUnYOHsSdhd/RbPu1qi5bmvBh4qccx0WwENNdAiSrwjG6aKUoeM7zKSDTiZn+MoTzCZbkQnq0xlnmWKZxntupxLM6+lw00ST+ZInDeJZsCfSpHIJDidFgZn7m+ATEYTWHICFs6ehL18p2uTsFUgIt3AncCNqjq1+LjptjIaaqBPTvVy8w/exEQOpvIB4/bpHNIAABzQSURBVPk8h9wDzMk4s9mTjRQFgMnsMX44/hI2jHXz5OTrOW/fa/BVyAXCrOfws+zyk4+twqPDuRKbOkps8nh0deXo/ETRb7EMDfc3n4VNwK4eIhInNM5fVdVvN1uedqahBnooP8xnT/5t9Gl+l1vzOs+8N8zj3h0A3J8utVzONqGsRaqdgAXYfzwou2KjVbj2obuaLQISTpp8GTigqp9ptjztToOj2SmtZ/TmO4hWk8tYRWwCdvW4GvhD4AkRmX9e+7iq3tNEmdqWtg03ahi1YhOwq4eqPgAlAr4YNSGqjXMxhDPmbsPaaz38R1YrIp3pdvV0C6bf1dTv7o7N+t/Ov76qc7Z0zFXdztv331H1OdXiBbdXfU7MeU9Z3a7qPmXDMAyjdsxAG4ZhtCjmgzaMFfAPF7+vZHmpR/DJbKpETfjDp75Ssnw1Kfco/v5N/16y/Paxz6+mOEYZbARtGIbRopiBNgzDaFHMxbFG2J3ayF9ccPZM+Puern5GeaXctue9FdVrhmyG0W7YCNowDKNFaegIOuUMcF7HGzgVHGIq8zyoVzZF1WqSiG2jO7GVuHTQx2bcAjWkZY4J/zi+ZpnNnWq5TN+GYawfKgnYXzJDgohsAL4J7AaOAO9S1fGlrrUpHuOPtm7l3lNb+Vl8hrw/i+dP0Nht1sI5qUt5lVxAf8Lhoh6PrtiZ7NdDmThPTJzPuJ/j8cTPmcosnUrLWN/80YHbmi1CTcScylxRRnOpZARdMkMC8D7gPlW9WURuAm4CPrpkYxLQn8izIdFBr5xD1p0h7SUJNL+kAKoeQZBD8VHNsnyAJUEkieAiEsNxEgtHHImzyd/Mhi6H/rgykPDojJ0ZxWcDYUMiDrkE8aBzmXZWjojcCrwNGFLVl0dlVXd+hmGsPSrJqFIuQ8L1hKmwAG4H7mcZAw1hxu5XDvhsy1yOr5APIFjG3s4H9J+RKQazvyLnnVqyvuN0s7vj19kYbGKb28W2DnchOIAjsCGh9CXyJB0l4Z49eu+Le1zcJ0zlXQ6Mn8Mov1rullbKbcAXCZ9S5rmJKju/kheucMKuGdRbtvc9fWtdr2cYrUBVPuhFGRK2RsabKLv3lkqu4YhyTkeGc4rzb5bl6GwHwdgGxrxuTrsHyS3jtnadFDuDc9iZSnJ+t3Jh9yyOVBZzJOX6bOvw6YjF6R7rrVzIGlHVvZFeC6mp8zMMY21RsYFenCEhCtVYyXkLqW02xrtqkZGeuM+5XQk2eimSc29itOO1S9bvpoOXdafoTyi98dr820kn4JLkBhz9g5rOL8X+9D9WWrWmzs8w2pGjmZG29eUvpt6+/YoMdJkMCadFZHtkQLYDQ6XOLUxtc37HpppC5/XH87yi3yMALg8clOWH3zFnDofQpVLp6LmQlOvzH7akeb1WMdRfhv0H63YpoD6dn2EYrUslqzjKZUi4G3gvcHP0+t1VkZDQLbJgZJ1g6cp1JOU2LYh/wzo/wzBal0o2qsxnSHijiDwa/b2V0DBfKyKHgGujz0Z9mO/8YJU7P8NYDUTEFZFficj3mi1LO1PJKo6lMiS8qb7irD9E5OuEE4KbRGQQ+ARhZ3eHiLwfOAa8s3kSGkZNfAg4AKz+TPsaxmJxNBlVvaHMIev8jLZERHYCvw18Evhwk8VpaywWh2EY9eazwEcIdx4bK8AMtLFmEZFbRWRIRJ4sKNsgIveKyKHodaCZMq41RGR+V+wjy9T7gIjsE5F9DRKtLTEDbaxlbgOuW1Q2v0vzIuC+6LNRP64G3i4iR4BvEC4u+KfFlVT1FlW9YjUT/a4FzEAbaxZV3QuMLSq+nnB3JtHr7zZUqDWOqn5MVXeq6m7g3cCPVfU9TRarbbFJQmO9UfEuzcKNQIbRDMxAG0YZCjcCidSwHXWdo6r3E8aRMWrEXBzGeuN0tDuTpXZpGkYrIKqNGxiIyDAwC4w0rNHVYRO13cN5qrq53sLAgm6PRh9rla+VqPYeSuo2ihT4vYJY238NjBaEct2gqh9Z7uJrUL+VUHifjfrulmu/GTSq/bK6baiBBhCRfe0+c9vq99Dq8lVCPe6hcJcmcJpwl+Z3gDuAc4l2aarq4onEVZetHWj2fa739sF80MYaxnZpGu2O+aANwzBalGYY6Fua0Ga9afV7aHX5KqGV76GVZasnzb7P9d5+433QhmEYRmWYi8MwDKNFMQNtGIbRojTUQIvIdSLyrIgcjtagtjwisktEfiIiB0TkKRH5UFTeclHR2lG/0D5R59pVv8vRbP0vp1cJ+Xx0/HERuayObZf8fS+qc42ITBZklPrzerW/LKrakD/ABZ4DLgASwGPAnka1vwK5twOXRe97gIPAHuDTwE1R+U3Ap5osZ1vqN5L9N4DLgCcLyky/60D/legVeCvwfcLMTlcBD9Wx/ZK/70V1riHc7NTw/00jR9BXAodV9XlVzRGGIry+ge3XhKqeVNX90ftpwjQ+O2i9qGhtqV9om6hzbavf5Wiy/ivR6/XAVzTkQaB/frv+Slni990SrMhAV/nItwN4seDzIC2kiEqItg2/GniIRVHRgLJR0RpE2+t3Eabf5tIo/Vei14boftHvezGvE5HHROT7InJJvdsuR80GWkRc4EvAbxE+8t8gInuWOqVEWdus8RORbuBO4EZVnWpQm9V0gG2t30ZTgz/Z9Ls6VKLXVdf9Mr/v/YTxMi4FvkAYLqAh1LwOWkReB/yFqr4l+vwxAFX97+Xq98Xjvzgn1VGrrG3PgempEa0w4EzUAR4EriUcMTwM3KCqT5eq359IqOl2dXQbndPyxjjllJ7H65J4UdmEzpSs6wdz5S7vq2rdQ0NEduQX9b5uPdgSq/6hwa/hWzLqD5X97q5E4aUeO167uFJh0PMOx+Ufr3j9Cppsb674yQ9KRewqx4JvDkBE5n1zJY3IOakO023lVKXbM7i1itcQLkxdW7L8itS2orJ/yTxQsu7Y3GNlru6vVlS3h8OX1tPtf9r07qrPmcpV387tY58v+91diQ+6oscOLcg9NpBIrKC5dceyfrfCxJvjuRq+GeuX9eZPrgcnV+OiquqtxnXXCisx0IPAroLPO4ETKxPHKGDZDtA6v5qpaHBhmafPwq+04lpdL94MVmKgHwYuEpHzRSRBmCDy7vqIZWAd4GpSkW7VMk9XTQ2LB4wlqNkHraqeiPwp8ENCB9KtqvpU3SQzFjpA4DhhB/gHzRWpNi69aRnXbsSRv9lZVDY51VtvcWAN6bYFqdG/b5RiRbOyqnoPcE+dZGkq85P0jqOIBIgoqnLWcVVB1SEI5Kxjq4F1gKvHWtXtG3uLJwMBvnDqSxVf49c7/reS5Q+k/77SS1S9eMAoj2VUITS+qWSWRCJHZ/ccvTtP48Q9vLkUfi6Om8gT604T5GNMvHAOU5O9eL5LPl+8fKmerKUOsNUw3a4aFS8ewDKmL4sZ6Ih549yzfYTUa9LQ0UlieAidVqRHYPNGSM/RNT1Bei5cb+x5sVUfSRtGm2FzJ3VkXRro+dFyPJGne2ASN5lfOJad7GL6rk34nsvU5MuYTXeQTOTo7EgTi3v0bR1h+yXP4acT5Gc7UBW8TALfi+F5MfK5OKoO+XyMQIUgcFC1qK7GusH8+3VkXRjo+ScoVUFE6e2bonvzOJ07huHNe/D6tpF86pfokUmO/vhy/vJnV/BCOs0p5zQTnMLBxSXOhcEe7nrPXvgvbyCeHic1NYIzN4Mcfw6m83gjKbJDA/jZBHMTPeTzcfK5BNnc6rpCGkUlT6Iv/d4biwu/+uOiovh/Ll61NT33waKy3d/7ZPH1ghJy/GRZ0YwGsFb9+81iXRjoUjhuaCAkM4uTmIDJKbzxLkbHB3gmPcXz8iSTmSN4/vjCObnOOabH+umcHcGdm8KZmUQyaUjnCdIO6rmIGyAxn1gyhzjmWjPWH+bfrx/rwkAHgRO6GyJXw9RkL/l8nLHH9vDgrecylosxmXslM55wbM7jGf0x6dwIvj991nUms0f4v+69gVc9EuAr5APBAWKOEhNlz8AkrzjveVKdGXrPGSbWPYcGBe6Nf2ngTRvrjmpWa5TjgfStdZDEqBdr1kDPP47PG2dVwY/ez8x1kskm2X9qB584/vAS8QfOxvPHuXPqb7izINaVEKM7dT4d7gC/n34N23sm2KjjbOybIbZtDuICiRg45oc2jGbSkTi3qvr16PBWypo10IWrK1QFRQgiI/3sxBYGZzs4NB1jzhtdWTsoGW+CQAOensrww6O76T+5kwsHz6UnmcF1AlwniGp/b0VtGYaxvlizBhpCwxyo4KuzMILOenH+7WQP35r5Lp6fxvMnVtiKT94bIe+Nsde5g1+e6EHEwZUkjjjIGsrLW2pJ4St+5/6zPn/5VecV1fmTg5WFcejp/HhNchnGWmXNGuh5Y6J6ZtdfIubhSJjnKwg8wkBaQfmLVN4a4BME02SD6WVrG4ZhVMKaNdAAgQo5P0bei9GVzHDu7mPEkzl+Z7ab2PHf49hcnvtzd5H3hpstqmEUUc5nms4da7AkRrNYswZ6fmJQVcgHLq4T0LN9hPjANC89toORTAedsQ5+Pt5lBtowjJZkzRpoRxRE6Upm6O2cQyTghUcvRlV44MiF/GwoyWA2Tc4zl4RhGK3JmjPQ81HnRBQRpa93ip4Nk4yd3sQ9z1zC0dk4D0yO81j2nwk0i2q22SK3JKV2De550y+LymYO7jrr858c/PKqyWQY6401Z6DnjXM8ng+XuMV8VIVcPs6pTIzBuYAh9wR+MNlsUQ3DMJZkWQMtIrcCbwOGVPXlUdkG4JvAbuAI8C5VHS93jdVkfild4eeOVIatO06R7JlldmSA0VObeX54G3vHZjigDzGXH2qGqIZhGFVRyQj6NuCLwFcKym4C7lPVm6OcYzcBH62/eEtTuJSuEBEl2TtDcuMUc+O9zMx1MppJccR5hqnZZxstpmEsybv7/6Rk+Tcm/qbBkhitxrIGWlX3isjuRcXXA9dE728H7qcBBrqUX1RESSazuE5AZ9cc3QOTiKPkZjrJTPaw/6lL+LcTWzg2GzCVb6+wtCJyBJgmTNjpWW48w1hf1OqD3qqqJwFU9aSIbKnoLD3jI66WwpRUhRd0HJ+urjkSiRwDFwySfFUGHc1w4iev5PTpLdwzuIVbRr6GH8yimi998dbmDao6slwlQYnFzs5gX0rPQeBW1OhLLn+iqCz2ys6iso3/9fmKrmcYRvWs+iRhYe6xbcnUomNnG5B5V0Vh/ObF+L6zUMd1fVwnCCcEE3kyo/34D6bJTOzkuaPncXK2h+NzAb4/jeIVXcswjOZzTmIz//v2d1V1zueGi1cULcebE1dXVf+b+eoj+6lmqj5nKWo10KdFZHs0et4OlJ11K8w9tqenTyE0rvMjYZEgqucQRLuu548FQeF1wk0nWS+OHzh0JrJ0dqRJJHN09M4QS+XY+8Dr+LuD/UwGOcacCdIyxHD+F+1snBX4UZSz7e8iXS5Q2PltT6VKnG4YRjtTq4G+G3gvcHP0+t1aLjJvnOcp5/6YN87z0eg83yVQwXV9HCdAHEVcn+eme/lh+htraQnd1ap6InIh3Ssiz6jq3vmDhZ3fJb29lh2gCsy/vzqIyC7CBQXbCAPd3KKqn2uuVO1LJcvsvk44IbhJRAaBTxAa5jtE5P3AMeCdlTQWEMbG8PPOWcvjCl0ZIroQntMRPVMmASJKdyqN6/qIKJlskvHJPv7tyVdyKpPk/iGfQNOV332Lo6onotchEbkLuBLYu/RZRhVU5N+H8o/hnzj6tyXrb+i8tKhsNl86pEC9VmuIlH6Kqvdj9zJ4wJ+p6n4R6QEeEZF7VfXpRgqxVqhkFccNZQ69qdrGVIWcFyOTj5PzYwQIXoFP2RFFBOKOj6C4TlgWc3wSMY+Y49PdNUdH5xzpuU4mpno4PjnA/zia5VeZr6LqEw6I2h8R6QIcVZ2O3r8Z+Mty9RMdWXZfcvCsss7fLp4Q9LYVB+BJXPKfi8r2XfP1orKrPvWD5QU31jXR4oH5BQTTInIA2AGYga6Bhu4kdETpiOeIuz5+cHac5DAUaBjcPhbzFkbSTjQJmOrI4LgBic40bjLPzPPdHBrdwrHZTobdQ6jmGnkrjWArcJeIQPh/+pqqmoWsH0v6942VEy3PfTXwUHMlaV8aaqDj8TxbtwwTT+Rx43liiTzxrjRO3COxYRqnP4s4hGmiHCCVgJiL9g2Q3/mS8BqnjiBTMzz/2Q4+9XyGEzzJeOZwI2+jIajq80Dxc7JRL5b078PZk7B9bnczZGxbRKQbuBO4UVWnShw33VZAQw20OEqyM00skceN+bjJHPHeOZxkDndLFga6wY1BLMzhp6kONJHE27Adb8dVEHg4uTRuEDCXS3LI+yWZ3GAjb8FYI1Ti3y+chN2R3GKTsBUiInFC4/xVVf12qTqm28poqIF2Yx6dGyZ5ct+r+PmJHfgq5ANBgQ43IOUGOISuEIHQxQEkXJ+u2AsAzORfSsa/mJ+cSpHzftpI8VsaJ5Gn49yzVzsmri21MadUsPcHVkWmVqVa/75RORL65L4MHFDVzzRbnnansSPouEdy4xSPD23jC0MHmfaHmM4cJSgxyywUb1IpRPEJ3YiGUTVV+/dP5IbLrtgoRaWZ4utJNas1kvFzSpZnVx4O4WrgD4EnROTRqOzjqnrPSi+8Hmmogc5MdfPUj6/iqckU0/4QWX8qMs7FKy/M9Bqrhfn3Vw9VfQCWGV0ZFdNQA31oBn734Txj+X8lnTuxppbFGYZh1JuGGuh8MM3gzP2NbNIwjBanWvdRrdwTVLdhJ+72V91Gf7J0ot+lGJotH1dkzWVUWa/sH1SSH27LaH2GYZTBWb6KYRiG0QxsBG0YLUpv6mUly6cyB0uWJ2JbS5ZX89g91GZJLdY6NoI2DMNoUcxAG4ZhtChmoA3DMFoUM9CGYRgtihlowzCMFqWSjColU9iIyAbgm8Bu4AjwLlUdXz1RDaOZCFLi5/L/vuR/LVn7dy55vKhsx1/FS9aNP1s6boe84+Mly/Ur7y9Z/uw3XlqyfHHsdYCLXv+rknW33nxRyfLpzDMly43VpZIR9HwKm4uBq4APisge4CbgPlW9CLgv+mxUiYjcKiJDIvJkQdkGEblXRA5FrwPNlNEwjOawrIFW1ZOquj96Pw3Mp7C5Hrg9qnY78LurJeQa5zbgukVlVXd+F/f08sgb/+NZf4ZhtDdV+aAXpbDZGuUfm89DtqXMOR8QkX0ism9loq5NoiweY4uKrfMzDKPynYSLU9hEsXSXpTBzQp+7RV+b+n2mNMOUM80kI5ya20+gs7XIvpY5q/OL0jIZxppkZ3IzH971e1Wd88d3Hai6nVLJkZeinK9/KfJH0lWf0/kX5Y9VZKDLpLA5LSLbIwOyHRgqf4WQbR15PvaK0xyb6ufo7FYOT5/Dt/IvkMmZga6Fwrxu25KpJkuz1lEUr6j0w4f/vmTtD5dKk/ndatt8b5X19y5fZZ4flTswWWWbxmpSySqOcils7ib8Bt0cvS779XOdgJ5Umr5sB5u8GJM5h625X2PM6Viok/NnyeZP04g40SIJUvFtxJxk0bFAA7LeBH4wC+qV/HFW0VK1J1TU+RU+nezp7bMcB4axxqhkBF0yhQ2hYb5DRN5PmOjunctdKFAh58foTmQ5z/HZkopzYc95ZIPzF5zhj08k+KeJO8l5p2q4nerY3Pkq3td/BTs6c2eVqwoZ3+H+IXhSnmXKO8lU5jC1dRou4QMIqFZs5Kvu/IJAmM2ePYre94ZrqxG0rbniJ0tmrDIajIi4wD7guKq+rdnytCvLGuhlUti8qarWNFyTmYzlSbge/R1z7OofQ0RxJBwABpxHcqanIQa6n61cuWmc3RtGio7NZlO8OHcup6a347t5puWFKANMNQgiLo6EI3S/hK9dRL4OXANsEpFB4BPU0PkZRovxIcIVX73NFqSdaUq4UT9wUA1tvojioECA6wTs6pzl97rezES8smVimSDgGTnChH+cmJMkQScpurlQd9Hlukueu7vboTd5suSxRMzjkr4MnbEBpvMbGHYvxdfqvQiuCPHo8eAbE18oOq6qN5Q5tbrOzzBaBBHZCfw28Engw00Wp61pioFWFXx1UF8QUXACnGiMfu7AKH/UW9lERaDCRLqT7xx7CQdmt9GtSXpiLpuSDtduH2f7MtcRURJuabdD3PW4bNtxLlVnQeaV8o0HVnwJw2gHPgt8BOgpV6Fwgnsg1t0gsdqPxhpoAUcU1wlQX3CcYMG1MY/rBLhOrswFivF8ly0pZTbfRUdM6IrBhkRAdyJLKl75dUoRc31iltS2bRGRW4G3AUOq+vKozEIUrCIiMq/vR0TkmnL1Cie4d6W22AR3GRpqoAUl7nrEXdBYgYtDav//dCay/IetQ1yxMY7rKK4ExB2f/s65eondFjiO0pnINluMVuM24IuEsWTmmd+lebOI3BR9/mgTZFurXA28XUTeCqSAXhH5J1V9T5Plaksaa6AlHCHXk5jrs7VCl4ixvlDVvdHu10KuJ5yUhXCX5v2Yga4bqvox4GMA0Qj6/zTjXDuWk9BYb1S8S7PQT2oYzcAMtGGUodBPKrICP9w6RVXvJ3xCMWrEAvYb643T0e5MKg1RYBjNQrSGtb01NyYyDMwCxTtD2otN1HYP56nq5noLAwu6PRp9rFW+VqLaeyip28gH/b2CVRx/DYwWTBJuUNWPLHfxNajfSii8z0Z9d8u13wwa1X5Z3TbUQAOIyD5VvaKhjdaZVr+HVpevEupxD4W7NIHThLs0vwPcAZxLtEtTVReHe1112dqBZt/nem8fzAdtrGFsl6bR7pgP2jAMo0VphoG+pQlt1ptWv4dWl68SWvkeWlm2etLs+1zv7TfeB20YhmFUhrk4DMMwWpSGGmgRuU5EnhWRw9ESp5ZHRHaJyE9E5ICIPCUiH4rKN4jIvSJyKHodaAFZ206/EAY1EpEhEXmyoMz02yCarf/l9Cohn4+OPy4il9Wx7ZK/70V1rhGRSRF5NPr783q1vyyq2pA/wAWeAy4AEsBjwJ5Gtb8CubcDl0Xve4CDwB7g08BNUflNwKeaLGdb6jeS/TeAy4AnC8pMv+tA/5XoFXgr8H3CxCFXAQ/Vsf2Sv+9Fda4hXEvf8P9NI0fQVwKHVfV5Vc0B3yAMXNPSqOpJVd0fvZ8mzBKxg1D226NqtwO/2xwJF2hL/UIY1AhYvBbZ9Nsgmqz/SvR6PfAVDXkQ6J/fDbpSlvh9twSNNNA7gBcLPg/SQoqohGhX2quBh1gUdAcoG3SnQbS9fhdh+m0ujdJ/JXptiO4X/b4X8zoReUxEvi8il9S77XI0cqNKqZQkbbOERES6gTuBG1V1Kkx23lK0tX7bANPv6lCJXldd94t/34sO7yfcjj0Txbn+DnBRPdsvRyNH0IPAroLPO4ETDWy/ZiRMy30n8FVV/XZU3GpBd9pWv2Uw/TaXRum/Er2uqu7L/L4XUNUpVZ2J3t8DxEVkU73aX4pGGuiHgYtE5HwRSQDvBu5uYPs1IeFQ+cvAAVX9TMGhu4H3Ru/fC3y30bItoi31uwSm3+bSKP1Xote7gf8lWs1xFTA5735ZKUv8vgvrbIvqISJXEtrN0Xq0vyyNnJEknI09SDhr+1+bMStag8y/Tvg49TjwaPT3VmAjcB9wKHrd0AKytp1+I7m/DpwE8oSjpfebfteP/kvpFfhj4I+j9wJ8KTr+BHBFHdsu9/subP9PgacIV5g8CLy+Uf8b20loGIbRothOQsMwjBbFDLRhGEaLYgbaMAyjRTEDbRiG0aKYgTYMw2hRzEAbhmG0KGagDcMwWhQz0IZhGC3K/w8+bH8lT/wovAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "f,axarr = plt.subplots(3,4)\n",
    "FIRST_IMAGE = 0\n",
    "SECOND_IMAGE = 23\n",
    "THIRD_IMAGE = 28\n",
    "CONVOLUTION_NUMBER = 1\n",
    "from tensorflow.keras import models\n",
    "\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = tf.keras.models.Model(inputs = model.input,outputs = layer_outputs)\n",
    "\n",
    "for x in range(0,4):\n",
    "    f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1,28,28,1))[x]\n",
    "    axarr[0,x].imshow(f1[0,:,:, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "    axarr[0,x].grid(False)\n",
    "    \n",
    "    f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "    axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "    axarr[1,x].grid(False)\n",
    "    \n",
    "    f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "    axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "    axarr[2,x].grid(False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.练习\n",
    "* 1.尝试编辑卷积数。把32改成16或者64。这对准确性或训练时间有什么影响?\n",
    "* 2.删除最后的卷积。 这将对准确性或训练时间产生什么影响？\n",
    "* 3.增加更多的卷积怎么样?你认为这会有什么影响?\n",
    "* 4.除去除第一个之外的所有卷积。你认为这会有什么影响?\n",
    "* 5.在上一课中，您实现了一个回调来检查loss函数，并在达到一定数量时取消训练。看看你能否在这里实现它!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 32s 533us/sample - loss: 0.1326 - accuracy: 0.9595\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 32s 532us/sample - loss: 0.0455 - accuracy: 0.9858\n",
      "Epoch 3/10\n",
      "  544/60000 [..............................] - ETA: 36s - loss: 0.0223 - accuracy: 0.9941"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-f57fe740732b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m ])\n\u001b[0;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\dl\\minconda3\\envs\\ml_dl_andrewng\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32md:\\dl\\minconda3\\envs\\ml_dl_andrewng\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\dl\\minconda3\\envs\\ml_dl_andrewng\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\dl\\minconda3\\envs\\ml_dl_andrewng\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\dl\\minconda3\\envs\\ml_dl_andrewng\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\dl\\minconda3\\envs\\ml_dl_andrewng\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\dl\\minconda3\\envs\\ml_dl_andrewng\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\dl\\minconda3\\envs\\ml_dl_andrewng\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\dl\\minconda3\\envs\\ml_dl_andrewng\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\dl\\minconda3\\envs\\ml_dl_andrewng\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32md:\\dl\\minconda3\\envs\\ml_dl_andrewng\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 练习1测试\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=10)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 把卷积数从32改成64：训练数据变长，训练集精度到达:99%，测试集精度到达98%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
